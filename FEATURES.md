# Wildfire Impact - Feature Overview

## ğŸ¯ Project Summary

**Wildfire Impact** is a comprehensive machine learning framework for **multi-hazard change detection** using satellite imagery. It extends the FLOGA dataset framework to support both **wildfire burnt area mapping** and **drought vegetation stress detection** through a unified, configurable pipeline.

---

## âœ¨ Core Features

### 1. **Multi-Hazard Detection** ğŸŒ
- **Wildfire Detection**: Identify burnt areas from bi-temporal satellite imagery
- **Drought Detection**: Detect vegetation stress and drought-affected regions
- **Unified Framework**: Single codebase handles multiple event types
- **Dynamic Configuration**: Switch between hazards via config parameter
- **Event-Type Filtering**: Automatically filters dataset based on event type

**Key Implementation**:
```python
"event_type": "wildfire"  # or "drought" - controls entire pipeline
```

---

### 2. **Multi-Model Architecture** ğŸ¤–
Support for **10 state-of-the-art change detection models**:

| Model | Type | Key Features |
|-------|------|--------------|
| **BAM-CD** | CNN | Siamese encoder with SCSE attention (proposed) |
| **U-Net** | CNN | Classic encoder-decoder with skip connections |
| **FC-EF-Conc** | CNN | Early fusion with concatenation |
| **FC-EF-Diff** | CNN | Early fusion with difference operation |
| **SNUNet** | CNN | Dense connections with channel attention |
| **HFANet** | CNN | Hierarchical feature aggregation |
| **ChangeFormer** | Transformer | Vision transformer with multi-scale prediction |
| **BIT-CD** | Transformer | Binary transformer for change detection |
| **ADHR-CDNet** | Hybrid | Deformable convolutions + recurrence |
| **TransUNet-CD** | Hybrid | CNN-Transformer hybrid architecture |

**Features**:
- Modular architecture system
- Easy to add new models
- Pre-trained weight support
- Multi-scale inference capability
- Flexible backbone selection (ResNet, DenseNet, etc.)

---

### 3. **Comprehensive Dataset Support** ğŸ“¡

#### **Satellite Data**:
- **Sentinel-2**: 10m, 20m, 60m resolution bands
- **MODIS**: 500m resolution data
- **Flexible Band Selection**: Configure which bands to use
- **Multi-Temporal**: Before/after event imagery

#### **Spatial Coverage**:
- **FLOGA (Wildfire)**: Greece, 2017-2021, 326 events
- **Extensible**: Support for custom drought datasets
- **Configurable Resolution**: 256Ã—256, 512Ã—512, or custom patch sizes

#### **Metadata & Masks**:
- Cloud masks (before and after)
- Water/sea masks
- Corine Land Cover classification
- Event-type metadata (wildfire/drought)
- Positive/negative sample flags

---

### 4. **Dynamic Class Label System** ğŸ·ï¸

#### **Automatic Label Switching**:

**Wildfire Mode**:
```
Class 0: Unburnt
Class 1: Burnt
Class 2: Other events (ignored)
```

**Drought Mode**:
```
Class 0: No drought
Class 1: Drought-affected
Class 2: Mixed/Uncertain (ignored)
```

**Implementation**:
```python
def get_class_labels(event_type='wildfire'):
    if event_type == 'drought':
        return CLASS_LABELS_DROUGHT
    else:
        return CLASS_LABELS_WILDFIRE
```

**Impact**:
- Output metrics use appropriate terminology
- WandB logging shows event-type-specific names
- No code changes needed to switch event types
- All downstream components auto-adapt

---

### 5. **Flexible Training Pipeline** ğŸš‚

#### **Training Modes**:
- **Full Training**: Train from scratch with custom hyperparameters
- **Transfer Learning**: Load pre-trained weights and fine-tune
- **Resume Training**: Continue from checkpoint
- **Evaluation Only**: Load checkpoint and evaluate

#### **Advanced Training Features**:
- **Multi-Repetition Experiments**: Run same experiment N times for statistical robustness
- **Mixed Precision Training**: Faster training with reduced memory (AMP)
- **Weighted Loss**: Automatic class weight computation for imbalanced data
- **Custom Loss Functions**: BCE, Dice, Combined BCE+Dice, Focal Loss
- **Learning Rate Scheduling**: Cosine annealing, step decay, linear warmup
- **Checkpoint Management**: Save best model, periodic checkpoints, resume training
- **Gradient Accumulation**: Handle larger effective batch sizes
- **Data Augmentation**: Flips, rotations, elastic deformations (configurable)

---

### 6. **Comprehensive Evaluation Metrics** ğŸ“Š

#### **Per-Class Metrics**:
- **Accuracy**: Pixel-wise classification correctness
- **Precision**: True positive rate among predictions
- **Recall**: True positive rate among ground truth
- **F1-Score**: Harmonic mean of precision and recall
- **IoU**: Intersection over Union (Jaccard index)

#### **Aggregate Metrics**:
- **Mean F1**: Average across both classes
- **Mean IoU**: Average IoU across classes
- **Confusion Matrix**: Full class breakdown
- **Balanced Accuracy**: Average of per-class recall

#### **Specialized Metrics**:
- **Land Cover Breakdown**: Metrics per Corine Land Cover class
- **Event-Type Specific**: Different labels and interpretations by hazard type
- **Multi-Scale Metrics**: For models supporting multi-scale prediction

**Output Example (Wildfire)**:
```
(0) test F-Score (Unburnt): 93.01
(0) test F-Score (Burnt): 87.45
(0) test IoU (Unburnt): 86.93
(0) test IoU (Burnt): 78.23
Mean f-score: 90.23 (std: 3.28)
Mean IoU: 82.58 (std: 4.35)
```

---

### 7. **Rapid Prototyping with Synthetic Data** âš¡

#### **Features**:
- **No Download Required**: Generate test data in minutes
- **Realistic Patterns**:
  - Wildfire: Circular burnt areas, darker pixels
  - Drought: Rectangular stress zones, altered spectral bands
- **Quick Testing**: Validate entire pipeline in 5 minutes
- **Configurable Event Type**: Generate wildfire or drought patterns

#### **Usage**:
```bash
# Generate wildfire synthetic dataset
python create_synthetic_dataset.py --event_type wildfire

# Generate drought synthetic dataset
python create_synthetic_dataset.py --event_type drought

# Run evaluation
python run_experiment.py --config configs/config_eval_synthetic.json
```

#### **Benefits**:
- Fast iteration during development
- No large dataset downloads needed
- Validates complete pipeline
- Tests both event types instantly

---

### 8. **Experiment Tracking & Logging** ğŸ“ˆ

#### **Console Output**:
- Per-epoch loss and metrics
- Progress bars with ETA
- Real-time F1-score tracking
- Comprehensive final results

#### **File-Based Logging**:
- Training logs with full history
- Best checkpoint selection
- Hyperparameter recording
- Results summary files

#### **Weights & Biases Integration** (Optional):
- Real-time metric tracking
- Experiment comparison
- Hyperparameter visualization
- Model checkpointing in cloud
- Automatic runs synchronization

**Configuration**:
```json
{
    "wandb": {
        "activate": true,
        "wandb_project": "wildfire-impact",
        "wandb_entity": "your-username"
    }
}
```

---

### 9. **Visualization & Analysis Tools** ğŸ¨

#### **Built-in Visualizations**:
- **Training Curves**: Loss and metrics over epochs
- **Prediction Overlays**: Predictions on satellite imagery
- **Error Analysis**: True Positives, False Positives, False Negatives separately
- **Confusion Matrices**: Per-class and aggregate
- **RGB/NRG Composites**: Red-Green-Blue and NIR-Red-Green band combinations
- **Sample Predictions**: On training/validation/test sets

#### **Visualization Script** (`visualize_predictions.py`):
```bash
python visualize_predictions.py \
    --results_path results/visualizations/ \
    --config configs/config.json \
    --bands nrg \
    --mode test \
    --export_numpy_preds
```

#### **Output**:
- High-resolution prediction maps
- Separate folders for TP/FP/FN cases
- NumPy arrays for further analysis
- Confidence visualizations

---

### 10. **Configuration System** âš™ï¸

#### **Hierarchical Configuration**:
- **Main Config** (`config.json`): Dataset, training, paths, event type
- **Method Config** (`method/*.json`): Model-specific hyperparameters
- **JSON5 Support**: Comments and flexible syntax
- **Validation**: Automatic config integrity checking

#### **Key Configuration Parameters**:

```json
{
    "method": "bam_cd",
    "mode": "train",
    "event_type": "wildfire",
    "dataset_type": "sen2_20_mod_500",
    
    "train": {
        "n_epochs": 150,
        "rep_times": 10,
        "loss_function": "dice+ce",
        "weighted_loss": true,
        "mixed_precision": false,
        "log_landcover_metrics": false
    },
    
    "datasets": {
        "data_source": "sen2",
        "batch_size": 8,
        "augmentation": true,
        "scale_input": "clamp_scale_10000",
        "selected_bands": {...}
    },
    
    "paths": {
        "dataset": "data/processed/",
        "results": "results/",
        "load_state": null
    }
}
```

#### **Customization Options**:
- 50+ configuration parameters
- Per-model customization
- Loss function selection
- Optimizer configuration
- Learning rate scheduling
- Data augmentation policies
- Band selection

---

### 11. **Data Processing Pipeline** ğŸ”„

#### **Complete Workflow**:

```
Raw HDF5 Files
    â†“
[Stage 1] Patch Extraction & Padding
    â†“
Cropped Patches (.npy files)
    â†“
[Stage 2] Train/Val/Test Splitting
    â†“
Split Metadata (.pkl files)
    â†“
[Runtime] Loading & Preprocessing
    â”œâ”€ Image loading
    â”œâ”€ NaN handling
    â”œâ”€ Normalization/Scaling
    â”œâ”€ Augmentation (training only)
    â†“
Batched Tensors
    â†“
Model Processing
```

#### **Data Scaling Options**:
- **Z-Score Normalization**: (x - mean) / std
- **Min-Max Scaling**: (x - min) / (max - min)
- **Clamp & Scale**: Clamp at value, then scale
- **Custom Range**: Map to arbitrary [min, max]

#### **Augmentation** (Training Only):
- Horizontal flip (50% probability)
- Vertical flip (50% probability)
- Random rotation (-15Â° to +15Â°)
- Elastic deformations (optional)

---

### 12. **Backward Compatibility & Robustness** ğŸ”„

#### **Features**:
- **Graceful Degradation**: Works with datasets missing new fields
- **Default Values**: Sensible defaults for all optional parameters
- **Legacy Support**: Compatible with original FLOGA format
- **Version Handling**: Handles different data formats

**Example**:
```python
# Safely handles datasets without event_type field
event_type = configs.get('event_type', 'wildfire')  # Default to wildfire
```

---

### 13. **Testing & Validation** âœ…

#### **Testing Infrastructure**:
- **Test Scripts**: Multiple test levels (unit, integration, system)
- **Synthetic Datasets**: For rapid validation without real data
- **Configuration Tests**: Verify config validity before training
- **Dataset Tests**: Check data loading and augmentation
- **Model Tests**: Validate architecture and forward pass

#### **Testing Guide** (`TESTING_GUIDE.md`):
- **Test 0**: Synthetic data generation
- **Test 1**: Environment setup verification
- **Test 2**: Configuration validation
- **Test 3**: Dataset loading
- **Test 4**: Model architecture
- **Test 5**: Training pipeline
- **Test 6**: Evaluation metrics
- **Test 7**: Visualization output
- **Test 8**: Land cover analysis

#### **Quick Start Options**:
- **5-minute test**: Synthetic data + quick evaluation
- **15-minute test**: Full pipeline with both event types
- **30+-minute test**: Complete validation with all metrics

---

### 14. **Project Organization** ğŸ“

#### **Directory Structure**:
```
Wildfire_Impact/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ config_eval_synthetic.json
â”‚   â”œâ”€â”€ config_eval_synthetic_drought.json
â”‚   â””â”€â”€ method/
â”‚       â”œâ”€â”€ bam_cd.json
â”‚       â”œâ”€â”€ unet.json
â”‚       â””â”€â”€ ... (other models)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bam_cd/
â”‚   â”œâ”€â”€ unet.py
â”‚   â”œâ”€â”€ snunet.py
â”‚   â””â”€â”€ ... (10+ models)
â”‚
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ bce_and_dice.py
â”‚   â””â”€â”€ dice.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ sen2_20_mod_500/
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ <model_name>/
â”‚       â””â”€â”€ <timestamp>/
â”‚
â”œâ”€â”€ Main Scripts:
â”œâ”€â”€ create_dataset.py
â”œâ”€â”€ create_synthetic_dataset.py
â”œâ”€â”€ run_experiment.py
â”œâ”€â”€ visualize_predictions.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ dataset_utils.py
â””â”€â”€ cd_experiments_utils.py
```

---

### 15. **Model Portability** ğŸš€

#### **Features**:
- **Checkpoint Save/Load**: Complete training state persistence
- **Pre-trained Weights**: Load and fine-tune
- **Model Architecture**: JSON-based configuration
- **Reproducibility**: Fixed seeds and deterministic operations
- **Cross-Model Compatibility**: Same checkpoint format across all models

**Example**:
```bash
# Train with BAM-CD
python run_experiment.py --config configs/config.json  # method: bam_cd

# Evaluate with same checkpoint
# (just change mode to "eval" and set load_state path)
```

---

### 16. **Performance Optimization** âš¡

#### **Training Speedups**:
- **Mixed Precision Training**: 2-3x faster with AMP
- **Batch Normalization**: Faster convergence
- **GPU Utilization**: Optimized for CUDA
- **Data Loading**: Multi-worker DataLoader
- **Memory Efficiency**: Gradient checkpointing support

#### **Inference Optimization**:
- **Batch Processing**: Efficient inference on multiple samples
- **Multi-Scale**: Some models support multi-scale inference
- **Model Quantization**: Support for quantized models (future)
- **TorchScript**: Export to production format (compatible)

---

### 17. **Documentation** ğŸ“š

#### **Comprehensive Guides**:
- **README.md**: Quick start and overview
- **EXPLANATION.md**: Detailed code explanation (1000+ lines)
- **TESTING_GUIDE.md**: Complete testing procedures
- **OUTPUT_EXPLAINER.md**: Interpreting results and logs
- **FEATURES.md**: This file - feature overview
- **LICENSE**: Open source license
- **DATA_LICENSE**: Data usage restrictions

#### **Code Documentation**:
- Docstrings for all major functions
- Type hints for function parameters
- Inline comments for complex logic
- Configuration schema documentation

---

## ğŸ“ Advanced Features

### **Multi-Repetition Experiments**
Run same experiment multiple times for statistical significance:
```json
{
    "train": {
        "rep_times": 10
    }
}
```
Results are averaged across all repetitions with standard deviations.

### **Class Balancing**
Multiple strategies for imbalanced data:
- **Class Weights**: Automatic weight computation
- **Oversampling**: OverSampler for positive samples
- **Focal Loss**: Alternative loss function for extreme imbalance
- **Dice Loss**: Naturally handles imbalance

### **Transfer Learning**
Fine-tune pre-trained models:
```json
{
    "paths": {
        "load_state": "path/to/pretrained.pt"
    }
}
```

### **Custom Loss Functions**
- Cross-Entropy with class weighting
- Dice Loss (segmentation-specific)
- Combined BCE + Dice
- Focal Loss for extreme imbalance

### **Learning Rate Scheduling**
- Cosine annealing
- Step-based decay
- Linear decay
- Custom schedules

---

## ğŸ“Š Statistics & Metrics

### **Dataset Statistics** (FLOGA Wildfire):
- **Total Events**: 326 wildfire events
- **Time Period**: 2017-2021
- **Temporal Resolution**: 10-30 days post-fire
- **Spatial Coverage**: Multiple regions in Greece
- **Class Balance**: ~20-30% burnt pixels (naturally imbalanced)

### **Model Performance** (BAM-CD on FLOGA):
- **Mean F1-Score**: ~85-90%
- **Mean IoU**: ~75-80%
- **Training Time**: 2-4 hours (150 epochs)
- **Model Size**: ~100MB

---

## ğŸ”§ Technical Stack

### **Deep Learning**:
- PyTorch (main framework)
- TorchVision (image operations)
- TorchMetrics (metric computation)

### **Data Processing**:
- NumPy (numerical operations)
- Pandas (tabular data)
- scikit-learn (preprocessing)
- H5PY (HDF5 handling)

### **Utilities**:
- PyJSON5 (flexible JSON config)
- tqdm (progress bars)
- WandB (optional experiment tracking)
- einops (tensor operations)

### **Hardware Support**:
- CUDA 11.0+ (GPU acceleration)
- CPU-only mode (slower but works)
- Mixed precision training (AMP)

---

## ğŸš€ Quickstart Examples

### **5-Minute Wildfire Test**:
```bash
python create_synthetic_dataset.py --event_type wildfire
python run_experiment.py --config configs/config_eval_synthetic.json
```

### **5-Minute Drought Test**:
```bash
python create_synthetic_dataset.py --event_type drought
python run_experiment.py --config configs/config_eval_synthetic_drought.json
```

### **Train Custom Model**:
```bash
# Edit config.json with your settings
python run_experiment.py --config configs/config.json
```

### **Evaluate Checkpoint**:
```bash
# Edit config to set mode: "eval" and load_state path
python run_experiment.py --config configs/config.json
```

### **Visualize Results**:
```bash
python visualize_predictions.py \
    --results_path results/visualizations/ \
    --config configs/config.json \
    --bands nrg \
    --mode test
```

---

## ğŸ¯ Use Cases

1. **Wildfire Impact Assessment**: Map burnt areas from satellite data
2. **Drought Monitoring**: Detect vegetation stress during dry periods
3. **Change Detection Research**: Benchmark new CD algorithms
4. **Environmental Monitoring**: Long-term hazard impact tracking
5. **Model Development**: Test new architectures against FLOGA
6. **Transfer Learning**: Fine-tune on custom datasets
7. **Multi-Hazard Analysis**: Compare performance across event types

---

## ğŸ“ˆ Future Extensions

Potential areas for expansion:
- Additional hazard types (floods, landslides, etc.)
- Real-time inference capability
- Model ensemble methods
- Uncertainty estimation
- Weakly-supervised learning
- 3D temporal modeling
- Cloud-native deployment
- API endpoints

---

## âœ… Summary

**Wildfire Impact** provides a complete, production-ready framework for multi-hazard change detection with:
- âœ… 10 state-of-the-art models
- âœ… Multi-hazard support (wildfire & drought)
- âœ… Dynamic configuration system
- âœ… Comprehensive evaluation metrics
- âœ… Rapid prototyping tools
- âœ… Detailed documentation
- âœ… Experiment tracking
- âœ… Full backward compatibility
- âœ… Testing infrastructure
- âœ… Professional-grade code quality

---

*Updated: January 24, 2026*
*Framework: Wildfire & Drought Detection*
*Models: 10 architectures, 50+ configurations*
